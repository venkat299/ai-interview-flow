LLM_PROVIDER=local
# Optional: leave these blank or provide keys if switching providers
OPENAI_API_KEY=
OPENAI_MODEL=gpt-3.5-turbo
GEMINI_API_KEY=
# When running LM Studio on the host machine, containers must use
# host.docker.internal to reach it.
LOCAL_LLM_URL=http://host.docker.internal:1234/v1/chat/completions
LLM_TIMEOUT=10.0
