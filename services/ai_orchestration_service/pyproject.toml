[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "ai_orchestration_service"
version = "0.1.0"
dependencies = [
    "fastapi>=0.116.1",
    "uvicorn[standard]>=0.35.0",
    "httpx>=0.28.1",
    "pydantic>=2.11.7",
    "pydantic-settings>=2.10.1",
    "pytest>=8.4.1",
    "pytest-asyncio>=1.1.0",
    "uvicorn>=0.35.0",
    "logging>=0.4.9.6",
]

# --- New Section for Application Settings ---
[tool.settings]
llm_provider = "local"
openai_api_key = ""
openai_model = "gpt-3.5-turbo"
gemini_api_key = ""
# Default URL for a locally hosted LLM like LM Studio.
# `host.docker.internal` allows Docker containers to access services
# running on the host machine.
local_llm_url = "https://248e110186a5.ngrok-free.app/v1/chat/completions"
llm_timeout = 10.0
